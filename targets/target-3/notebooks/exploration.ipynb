{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-intro",
   "metadata": {},
   "source": [
    "# Model exploration notebook\n",
    "\n",
    "Primary backbone: **meta-llama/Llama-3.1-8B**\n",
    "Reranker: **cross-encoder/ms-marco-MiniLM-L-6-v2**\n",
    "\n",
    "Baselines: `google/flan-t5-xxl` and `mistralai/Mistral-7B-v0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from huggingface_hub import InferenceClient, hf_hub_download\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-llm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base LLM -- multi-line open-paren (scanner stitches these)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline() positional\n",
    "gen_pipe = pipeline(\"text-generation\", \"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "# pipeline() model= kwarg\n",
    "ner_pipe = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-hub-download",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_hub_download repo_id= kwarg\n",
    "gguf = hf_hub_download(\n",
    "    repo_id=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
    "    filename=\"Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\",\n",
    ")\n",
    "print(f\"Downloaded to: {gguf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-inference-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InferenceClient positional\n",
    "client = InferenceClient(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "response = client.text_generation(\"Tell me a joke\", max_new_tokens=100)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-tricky",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable indirection -- NOT detectable\n",
    "BASELINE = \"google/flan-t5-xxl\"\n",
    "baseline_model = AutoModelForCausalLM.from_pretrained(BASELINE)\n",
    "\n",
    "# Multi-line split on open paren -- scanner stitches lines ending with '('\n",
    "alt = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    ")\n",
    "\n",
    "# Local path -- correctly NOT detected\n",
    "local = AutoModelForCausalLM.from_pretrained(\"./checkpoints/step-1000\")"
   ]
  }
 ]
}